<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog on 陈sir 的流水帐</title>
    <link>https://yudar1024.github.io/blog/</link>
    <description>Recent content in Blog on 陈sir 的流水帐</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 16 Oct 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://yudar1024.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>kubernetes 部署 rabbitmq 集群</title>
      <link>https://yudar1024.github.io/blog/deploy-rabbitmq-cluster/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/deploy-rabbitmq-cluster/</guid>
      <description>RABBITMQ 集群基础知识 队列镜像 rabbitmq的队列（queue）镜像，指master node 在接受到请求后，会同步到其他节点上，以此来保证高可用。在confirm模式下，具体过程如下
clientpublisher 发送消息&amp;ndash;&amp;gt; master node接到消息&amp;ndash;&amp;gt; master node 将消息持久化到磁盘 &amp;ndash;&amp;gt; 将消息异步发送给其他节点&amp;ndash;&amp;gt;master 将ack 返回给client publisher。
 需要理解的点，1 rabbitmq的消息接口是异步的。 publisher 发送给 master 之后，在没收到ack之前仍然可以继续发送消息，这中间没有阻塞。 2 master 在像其他节点同步消息的过程是阻塞性的，在这个工程中，队列不能读写。
 当master节点挂掉之后，集群是如何处理的  运行时间最长的镜像被提升为主镜像，假设它最有可能与主镜像完全同步。如果没有与主服务器同步的镜像，仅存在于主服务器上的消息将丢失。因为原master上的消息都已经落盘，所以也不会完全丢失。
 mirror认为之前所有的消费者都被突然断开连接。它需要重发所有已发送到消费者但正在等待消费者确认的消息。这可能是客户端发出确认的消息，例如，如果消费者的确认消息在到达节点master之前网络上丢失（断网），或者在从master发送到到镜像主机时丢失。在这两种情况下，新主服务器别无选择，只能重新发送所有它没有看到确认的消息。（要求消费者对信息具有幂等性）
 当队列故障转移的时候，如果有消息被取消，publisher将会收到通知。
 消费者需要注意，作为重建队列的代价，消费者可能会重复收到之前消费过的消息。
 当选择的镜像成为主镜像时，在此期间发布到镜像队列的消息不会丢失。发布到镜像队列的消息会被路由到新的master 节点，然后同步到所有的mirror 节点。如果主服务器挂掉，消息将继续发送到镜像，并在镜像升级为master服务器完成后添加到队列中。
 发布者如果使用confirm模式，即使master 挂掉，消息仍然会被后面mirror 节点确认（提升为master），在发布者的角度，confirm模式发布的消息，发布到集群和发布到单节点，没什么不同。
 当一个镜像节点加入，或重新加入镜像队列是，之前的队列内容全部丢失。
 当所有slave都处在(与master)未同步状态时，并且ha-promote-on-shutdown policy设置为when-syned(默认)时，如果master因为主动的原因停掉，比如是通过rabbitmqctl stop命令停止或者优雅关闭OS，那么slave不会接管master，也就是说此时镜像队列不可用；但是如果master因为被动原因停掉，比如VM 或者OS crash了，那么slave会接管master。这个配置项隐含的价值取向是优先保证消息可靠不丢失，放弃可用性。如果ha-promote-on- shutdown policy设置为alway，那么不论master因为何种原因停止，slave都会接管master，优先保证可用性。
 镜像队列中最后一个停止的节点会是master，启动顺序必须是master先起，如果slave先起，它会有30秒的等待时间，等待master启动， 然后加入cluster。当所有节点因故(断电等)同时离线时，每个节点都认为自己不是最后一个停止的节点。要恢复镜像队列，可以尝试在30秒之内同时启 动所有节点。
 对于镜像队列，客户端basic.publish操作会同步到所有节点；而其他操作则是通过master中转，再由master将操作作用于salve。 比如一个basic.get操作，假如客户端与slave建立了TCP连接，首先是slave将basic.get请求发送至master，由 master备好数据，返回至slave，投递给消费者。
 由10可知，当slave宕掉时，除了与slave相连的客户端连接全部断开之外，没有其他影响。当master宕掉时，会有以下连锁反应：1)与 master相连的客户端连接全部断开。2)选举最老的slave为master。若此时所有slave处于未同步状态，则未同步部分消息丢失。3)新的 master节点requeue所有unack消息，因为这个新节点无法区分这些unack消息是否已经到达客户端，亦或是ack消息丢失在到老 master的通路上，亦或是丢在老master组播ack消息到所有slave的通路上。所以处于消息可靠性的考虑，requeue所有unack的消 息。此时客户端可能受到重复消息。4)如果客户端连着slave，并且basic.</description>
    </item>
    
    <item>
      <title>spring security 与 keyclaok 集成，使用 oidc 登录</title>
      <link>https://yudar1024.github.io/blog/spring-security-with-oidc/</link>
      <pubDate>Wed, 25 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/spring-security-with-oidc/</guid>
      <description>核心代码 securityConfiguration.java 文件
package com.mycompany.gateway.config; import com.mycompany.gateway.security.*; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Import; import org.springframework.http.HttpMethod; import org.springframework.security.config.annotation.method.configuration.EnableGlobalMethodSecurity; import org.springframework.security.config.annotation.web.builders.HttpSecurity; import org.springframework.security.config.annotation.web.builders.WebSecurity; import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity; import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter; import com.mycompany.gateway.security.oauth2.AudienceValidator; import com.mycompany.gateway.security.SecurityUtils; import org.springframework.security.oauth2.core.DelegatingOAuth2TokenValidator; import org.springframework.security.oauth2.core.OAuth2TokenValidator; import org.springframework.security.oauth2.jwt.*; import com.mycompany.gateway.security.oauth2.AuthorizationHeaderFilter; import com.mycompany.gateway.security.oauth2.AuthorizationHeaderUtil; import org.springframework.beans.factory.annotation.Value; import org.springframework.security.core.GrantedAuthority; import org.springframework.security.core.authority.mapping.GrantedAuthoritiesMapper; import org.springframework.security.oauth2.core.oidc.user.OidcUserAuthority; import java.util.*; import org.springframework.security.web.csrf.CookieCsrfTokenRepository; import org.springframework.security.web.csrf.CsrfFilter; import com.mycompany.gateway.security.oauth2.JwtAuthorityExtractor; import org.springframework.security.web.header.writers.ReferrerPolicyHeaderWriter; import org.springframework.web.filter.CorsFilter; import org.zalando.problem.spring.web.advice.security.SecurityProblemSupport; @EnableWebSecurity @EnableGlobalMethodSecurity(prePostEnabled = true, securedEnabled = true) @Import(SecurityProblemSupport.class) public class SecurityConfiguration extends WebSecurityConfigurerAdapter { private final CorsFilter corsFilter; // 这里的key，需要和application.</description>
    </item>
    
    <item>
      <title>kubernetes service 介绍</title>
      <link>https://yudar1024.github.io/blog/service-introduction/</link>
      <pubDate>Mon, 23 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/service-introduction/</guid>
      <description>什么是service 是发现后端pod服务；
是为一组具有相同功能的容器应用提供一个统一的入口地址；
是将请求进行负载分发到后端的各个容器应用上的控制器。
对service的访问来源 访问service的请求来源有两种：k8s集群内部的程序（Pod）和 k8s集群外部的程序。
service类型 采用微服务架构时，作为服务所有者，除了实现业务逻辑以外，还需要考虑如何把服务发布到k8s集群或者集群外部，使这些服务能够被k8s集群内的应用、其他k8s集群的应用以及外部应用使用。因此k8s提供了灵活的服务发布方式，用户可以通过ServiceType来指定如何来发布服务，类型有以下几种：
● ClusterIP：提供一个集群内部的虚拟IP以供Pod访问（service默认类型)。
service 结构如下：  ● NodePort:在每个Node上打开一个端口以供外部访问
Kubernetes将会在每个Node上打开一个端口并且每个Node的端口都是一样的，通过:NodePort的方式Kubernetes集群外部的程序可以访问Service。
service 定义如下：  ● LoadBalancer：通过外部的负载均衡器来访问
service selector service通过selector和pod建立关联。
k8s会根据service关联到pod的podIP信息组合成一个endpoint。
若service定义中没有selector字段，service被创建时，endpoint controller不会自动创建endpoint。
service负载分发策略 service 负载分发策略有两种：
RoundRobin：轮询模式，即轮询将请求转发到后端的各个pod上（默认模式）； SessionAffinity：基于客户端IP地址进行会话保持的模式，第一次客户端访问后端某个pod，之后的请求都转发到这个pod上。  三、服务发现 k8s服务发现方式 虽然Service解决了Pod的服务发现问题，但不提前知道Service的IP，怎么发现service服务呢？
k8s提供了两种方式进行服务发现：
● 环境变量： 当创建一个Pod的时候，kubelet会在该Pod中注入集群内所有Service的相关环境变量。需要注意的是，要想一个Pod中注入某个Service的环境变量，则必须Service要先比该Pod创建。这一点，几乎使得这种方式进行服务发现不可用。
例如： 一个ServiceName为redis-master的Service，对应的ClusterIP:Port为10.0.0.11:6379，则其在pod中对应的环境变量为： REDIS_MASTER_SERVICE_HOST=10.0.0.11 REDIS_MASTER_SERVICE_PORT=6379 REDIS_MASTER_PORT=tcp://10.0.0.11:6379 REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379 REDIS_MASTER_PORT_6379_TCP_PROTO=tcp REDIS_MASTER_PORT_6379_TCP_PORT=6379 REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11  ● DNS：可以通过cluster add-on的方式轻松的创建KubeDNS来对集群内的Service进行服务发现————这也是k8s官方强烈推荐的方式。为了让Pod中的容器可以使用kube-dns来解析域名，k8s会修改容器的/etc/resolv.conf配置。
k8s服务发现原理 ● endpoint
endpoint是k8s集群中的一个资源对象，存储在etcd中，用来记录一个service对应的所有pod的访问地址。
service配置selector，endpoint controller才会自动创建对应的endpoint对象；否则，不会生成endpoint对象.
例如，k8s集群中创建一个名为k8s-classic-1113-d3的service，就会生成一个同名的endpoint对象，如下图所示。其中ENDPOINTS就是service关联的pod的ip地址和端口。  ● endpoint controller
endpoint controller是k8s集群控制器的其中一个组件，其功能如下：
负责生成和维护所有endpoint对象的控制器 负责监听service和对应pod的变化 监听到service被删除，则删除和该service同名的endpoint对象 监听到新的service被创建，则根据新建service信息获取相关pod列表，然后创建对应endpoint对象 监听到service被更新，则根据更新后的service信息获取相关pod列表，然后更新对应endpoint对象 监听到pod事件，则更新对应的service的endpoint对象，将podIp记录到endpoint中  四、负载均衡 kube-proxy kube-proxy负责service的实现，即实现了k8s内部从pod到service和外部从node port到service的访问。</description>
    </item>
    
    <item>
      <title>Keycloak 配置SSL 实现HTTPS 访问</title>
      <link>https://yudar1024.github.io/blog/keycloak-ssl-configuration/</link>
      <pubDate>Sat, 21 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/keycloak-ssl-configuration/</guid>
      <description># 生成keystore.jks keytool -genkeypair -alias keycloak.me -keyalg RSA -keystore keycloak.jks -validity 10950   注意：CN必须是主机名，也可以是ip但是ip容易变，所以主机名或域名。后面访问的是时候通过在hosts 就文件映射域名到IP, 这里将CN 设置为keycloak.me, 注意alias参数不是设置CN的地方
 生成好keycloak.jks 之后 复制到 keycloak-7.0.0\standalone\configuration\ 文件夹内
修改standalone.xml 或 standalone-ha.xml 或 domain.xml 具体修哪个文件，看你的安装方式，此处使用standalone.xml
在 下添加
&amp;lt;security-realm name=&amp;quot;ApplicationRealm&amp;quot;&amp;gt; &amp;lt;server-identities&amp;gt; &amp;lt;ssl&amp;gt; &amp;lt;keystore path=&amp;quot;application.keystore&amp;quot; relative-to=&amp;quot;jboss.server.config.dir&amp;quot; keystore-password=&amp;quot;password&amp;quot; alias=&amp;quot;server&amp;quot; key-password=&amp;quot;password&amp;quot; generate-self-signed-certificate-host=&amp;quot;localhost&amp;quot; /&amp;gt; &amp;lt;/ssl&amp;gt; &amp;lt;/server-identities&amp;gt; &amp;lt;authentication&amp;gt; &amp;lt;local default-user=&amp;quot;$local&amp;quot; allowed-users=&amp;quot;*&amp;quot; skip-group-loading=&amp;quot;true&amp;quot; /&amp;gt; &amp;lt;properties path=&amp;quot;application-users.properties&amp;quot; relative-to=&amp;quot;jboss.server.config.dir&amp;quot; /&amp;gt; &amp;lt;/authentication&amp;gt; &amp;lt;authorization&amp;gt; &amp;lt;properties path=&amp;quot;application-roles.properties&amp;quot; relative-to=&amp;quot;jboss.server.config.dir&amp;quot; /&amp;gt; &amp;lt;/authorization&amp;gt; &amp;lt;/security-realm&amp;gt; &amp;lt;!-- 添加的部分 --&amp;gt; &amp;lt;security-realm name=&amp;quot;UndertowRealm&amp;quot;&amp;gt; &amp;lt;server-identities&amp;gt; &amp;lt;ssl&amp;gt; &amp;lt;keystore path=&amp;quot;keycloak.</description>
    </item>
    
    <item>
      <title>日常软件使用技巧</title>
      <link>https://yudar1024.github.io/blog/software-knowledge/</link>
      <pubDate>Tue, 17 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/software-knowledge/</guid>
      <description> 换行符问题(LF/CRLF) 将VScode， idea ， sublime text 设置为默认使用 linux 下的换行符
 vscode： 设置&amp;ndash;》用户设置&amp;ndash;》文本编辑器&amp;ndash;》文件&amp;ndash;》eol， 设置为\n 或者搜索 files:eol 进行设置 idea： File&amp;ndash;》Settings&amp;ndash;》Editor&amp;ndash;》Code Style&amp;ndash;》Line separator 设置为 Unix and OS X（\n） sublime text： Perference-&amp;gt;Setting-User 中加入配置 &amp;quot;default_line_ending&amp;quot;: &amp;quot;unix&amp;quot; 可选项为 system， windows， unix  </description>
    </item>
    
    <item>
      <title>使用 heketi 维护 glusterfs</title>
      <link>https://yudar1024.github.io/blog/using-heketi-to-maintain-glusterfs/</link>
      <pubDate>Wed, 11 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/using-heketi-to-maintain-glusterfs/</guid>
      <description>glusterfs 最日常的运维工作就是添加磁盘，添加节点。
添加磁盘  识别磁盘  linux 生产环境，不一定能够随时停机，此时就要求新添加的硬盘能够被热识别。如果是x86体系的物理硬件，必须断电停机，然后加硬盘。所以不存在问题，但是现在大多数环境是虚拟环境，所以添加硬盘非常方便。不需要断电，也就需要系统能够热识别。
此时linux服务器已经有sda 与sdb 两块磁盘，添加sdc磁盘（未识别，sdc是后面会识别出来的号）
# fdisk -l # 查看当前设备状态。此时看不到新添加的设备sdc # #进入/sys/class/scsi_host目录，在/sys/class/scsi_host下找到符合指向本机iscsi设备主机符号链接表 # ls -al /sys/class/scsi_host total 0 drwxr-xr-x. 2 root root 0 Nov 12 15:31 . drwxr-xr-x. 45 root root 0 Nov 12 15:31 .. lrwxrwxrwx. 1 root root 0 Nov 12 15:31 host0 -&amp;gt; ../../devices/pci0000:00/0000:00:07.1/host0/scsi_host/host0 lrwxrwxrwx. 1 root root 0 Nov 12 15:31 host1 -&amp;gt; ../../devices/pci0000:00/0000:00:07.1/host1/scsi_host/host1 lrwxrwxrwx. 1 root root 0 Nov 12 15:31 host2 -&amp;gt; .</description>
    </item>
    
    <item>
      <title>kubernetes 使用独立 glusterfs &#43; heketi集群</title>
      <link>https://yudar1024.github.io/blog/deploy-glusterfs-heketi-out-of-k8s/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/deploy-glusterfs-heketi-out-of-k8s/</guid>
      <description>主机列表
   Hostname 服务器IP 存储IP 用途 硬盘信息 容量G     master1 192.168.10.50 192.168.10.50 hekteti server /dev/sdb 300G   node1 192.168.10.51 192.168.10.51 glusterfs /dev/sdb 300G   node2 192.168.10.52 192.168.10.53 glusterfs /dev/sdb 300G   node3 192.168.10.53 192.168.10.53 glusterfs /dev/sdb 300G    所有主机的/etc/hosts均包含
192.168.10.50 master1 192.168.10.51 node1 192.168.10.52 node2 192.168.10.53 node3  配置所有服务器之间免密登录（可选）
在node1，node2，node3 安装 glusterfs-server glusterfs-fuse，并加载fuse 内核模块
#!/bin/bash yum install -y centos-release-gluster yum install -y glusterfs-server glusterfs-fuse systemctl start glusterd systemctl enable glusterd modprobe fuse echo &amp;quot;modprobe -- fuse&amp;quot; &amp;gt;&amp;gt; /etc/sysconfig/modules/glusterfs.</description>
    </item>
    
    <item>
      <title>kubernetes 集群中部署 glusterfs 与 heketi</title>
      <link>https://yudar1024.github.io/blog/deploy-glusterfs-hekti-in-k8s/</link>
      <pubDate>Sat, 07 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/deploy-glusterfs-hekti-in-k8s/</guid>
      <description>组件介绍 Heketi Heketi提供了一个RESTful管理界面，可以用来管理GlusterFS卷的生命周期。 通过Heketi，就可以像使用OpenStack Manila，Kubernetes和OpenShift一样申请可以动态配置GlusterFS卷。Heketi会动态在集群内选择bricks构建所需的volumes，这样以确保数据的副本会分散到集群不同的故障域内。同时Heketi还支持任意数量的ClusterFS集群，以保证接入的云服务器不局限于单个GlusterFS集群。
Gluster-Kubernetes Gluster-Kubernetes是一个可以将GluserFS和Hekiti轻松部署到Kubernetes集群的开源项目。另外也提供在Kubernetes中可以采用StorageClass来动态管理GlusterFS卷。
部署环境 服务器分配信息:
   Hostname 服务器IP 存储IP 硬盘信息 容量G     master1 192.168.10.51 192.168.10.51 /dev/sdb 300G   node1 192.168.10.52 192.168.10.52 /dev/sdb 300G   node2 192.168.10.53 192.168.10.53 /dev/sdb 300G    部署步骤 安装依赖组件 （master1 node1 node2）
yum install -y centos-release-gluster yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma glusterfs-geo-replication glusterfs-devel systemctl start glusterd systemctl enable glusterd  内核模块加载 （master1 node1 node2）</description>
    </item>
    
  </channel>
</rss>