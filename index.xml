<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>陈sir 的流水帐</title>
    <link>https://yudar1024.github.io/</link>
    <description>Recent content on 陈sir 的流水帐</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 21 Oct 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://yudar1024.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>kafka 概念介绍</title>
      <link>https://yudar1024.github.io/blog/kafka-introduce/</link>
      <pubDate>Mon, 21 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/kafka-introduce/</guid>
      <description>kafka 基础概念 基础概念  Producer: 消息发送者 broker： kafka节点 consumer: 消费者 topic： 主题，一组消息的集合，分布在多个分区里 partition：分区， 一个分区只能属于一个主题。在存储曾铭，分区可以看做一个可追加log日志文件，每个消息在分区中表现为一条日志，而消息的位置，通过offset 来记录，offset 是消息在分区分区中的唯一标示，offset保证消息的顺序性，注意，offset 不跨分区，在不同的分区中，offset 可能相同。 replica：副本，对分区的冗余，会尽量分布在不同的节点。一个分区会有leader 和 follower 两种角色的副本。 AR（assignded-replica）：分区中的所有副本统称为AR ISR (in-sync replica)：与 leader 副本保持一定程度同步的副本成为 ISR， ISR 是AR 的子集。 OSR (out-of-sync replica)：与leader 同步滞后达到一定程度的副本，这个一定程度可以通过参数进行设置。AR=ISR+OSR HW(high watermark 高水位)：指一个特殊的OFFSET，消费者，只能获取HW 这个offset 之前的消息，即便HW 之后有其他消息，对消费者而言也是不可见的。（与ISR 强相关） LSO(logstartoffset)：消息开始的初始offset，值为0 LEO(logendoffset)：当前分区中，下一条待写入消息的offset。  offset消息的发送过程 producer-&amp;gt; 拦截器（非必须） -&amp;gt; 序列化器 -&amp;gt; 分区器- broker
producer-flow消息投递方式 P2P , PUB/SUB kafka 通过消费者组和消费者来适配两种模式
如果所有的消费者头隶属于同一个消费者组，那么消息只会投递给其中一个消费者，相当于P2P模式
如果消费者隶属于不同的消费者组，那么消息会发送给所有的消费者组。适配PUB/SUB模式
消费位移（offset） 指消费者在kafka中的消费位置，注意这个角度跟消息的offset 不是一个意思。消息的offset 是指消息在kafka中的偏移量，而消费位移，是指消费者在分区中将消息消费到了哪一条。消费者在消费完消息之后需要提交消费位移。注意提交的消费位移的值，当前消费过的最后一条消息的offset+1,而不是当前消费了的消息的offset</description>
    </item>
    
    <item>
      <title>kubernetes 部署 rabbitmq 集群</title>
      <link>https://yudar1024.github.io/blog/deploy-rabbitmq-cluster/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/deploy-rabbitmq-cluster/</guid>
      <description>RABBITMQ 集群基础知识 基础概念  Producer: 消息发送者
 RabbitMQ
  :
 Vhost: 相当于分组，每个vhost下数据是隔离的 Exchange: 路由器，接收消息，本根据RoutingKey分发消息  headers：消息头类型 路由器，内部应用 direct：精准匹配类型 路由器 topic：主题匹配类型 路由器，支持正则 模糊匹配 fanout：广播类型 路由器，RoutingKey无效  RoutingKey: 路由规则 Queue: 队列，用于存储消息（消息的目的地）
 Consumer: 消息消费者
  持久化 RabbitMQ持久化分为Exchange、Queue、Message
 Exchange 和 Queue 持久化 指持久化Exchange、Queue 元数据，持久化的是自身，服务宕机，Exchange 和 Queue 自身就没有了 Message 持久化 顾名思义 把每一条消息体持久化，服务宕机，消息不丢失  镜像队列 rabbitmq的队列（queue）镜像，指master node 在接受到请求后，会同步到其他节点上，以此来保证高可用。在confirm模式下，具体过程如下
clientpublisher 发送消息&amp;ndash;&amp;gt; master node接到消息&amp;ndash;&amp;gt; master node 将消息持久化到磁盘 &amp;ndash;&amp;gt; 将消息异步发送给其他节点&amp;ndash;&amp;gt;master 将ack 返回给client publisher。</description>
    </item>
    
    <item>
      <title>spring security 与 keyclaok 集成，使用 oidc 登录</title>
      <link>https://yudar1024.github.io/blog/spring-security-with-oidc/</link>
      <pubDate>Wed, 25 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/spring-security-with-oidc/</guid>
      <description>核心代码 securityConfiguration.java 文件
package com.mycompany.gateway.config; import com.mycompany.gateway.security.*; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Import; import org.springframework.http.HttpMethod; import org.springframework.security.config.annotation.method.configuration.EnableGlobalMethodSecurity; import org.springframework.security.config.annotation.web.builders.HttpSecurity; import org.springframework.security.config.annotation.web.builders.WebSecurity; import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity; import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter; import com.mycompany.gateway.security.oauth2.AudienceValidator; import com.mycompany.gateway.security.SecurityUtils; import org.springframework.security.oauth2.core.DelegatingOAuth2TokenValidator; import org.springframework.security.oauth2.core.OAuth2TokenValidator; import org.springframework.security.oauth2.jwt.*; import com.mycompany.gateway.security.oauth2.AuthorizationHeaderFilter; import com.mycompany.gateway.security.oauth2.AuthorizationHeaderUtil; import org.springframework.beans.factory.annotation.Value; import org.springframework.security.core.GrantedAuthority; import org.springframework.security.core.authority.mapping.GrantedAuthoritiesMapper; import org.springframework.security.oauth2.core.oidc.user.OidcUserAuthority; import java.util.*; import org.springframework.security.web.csrf.CookieCsrfTokenRepository; import org.springframework.security.web.csrf.CsrfFilter; import com.mycompany.gateway.security.oauth2.JwtAuthorityExtractor; import org.springframework.security.web.header.writers.ReferrerPolicyHeaderWriter; import org.springframework.web.filter.CorsFilter; import org.zalando.problem.spring.web.advice.security.SecurityProblemSupport; @EnableWebSecurity @EnableGlobalMethodSecurity(prePostEnabled = true, securedEnabled = true) @Import(SecurityProblemSupport.class) public class SecurityConfiguration extends WebSecurityConfigurerAdapter { private final CorsFilter corsFilter; // 这里的key，需要和application.</description>
    </item>
    
    <item>
      <title>kubernetes service 介绍</title>
      <link>https://yudar1024.github.io/blog/service-introduction/</link>
      <pubDate>Mon, 23 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/service-introduction/</guid>
      <description>什么是service 是发现后端pod服务；
是为一组具有相同功能的容器应用提供一个统一的入口地址；
是将请求进行负载分发到后端的各个容器应用上的控制器。
对service的访问来源 访问service的请求来源有两种：k8s集群内部的程序（Pod）和 k8s集群外部的程序。
service类型 采用微服务架构时，作为服务所有者，除了实现业务逻辑以外，还需要考虑如何把服务发布到k8s集群或者集群外部，使这些服务能够被k8s集群内的应用、其他k8s集群的应用以及外部应用使用。因此k8s提供了灵活的服务发布方式，用户可以通过ServiceType来指定如何来发布服务，类型有以下几种：
● ClusterIP：提供一个集群内部的虚拟IP以供Pod访问（service默认类型)。
service 结构如下：  ● NodePort:在每个Node上打开一个端口以供外部访问
Kubernetes将会在每个Node上打开一个端口并且每个Node的端口都是一样的，通过:NodePort的方式Kubernetes集群外部的程序可以访问Service。
service 定义如下：  ● LoadBalancer：通过外部的负载均衡器来访问
service selector service通过selector和pod建立关联。
k8s会根据service关联到pod的podIP信息组合成一个endpoint。
若service定义中没有selector字段，service被创建时，endpoint controller不会自动创建endpoint。
service负载分发策略 service 负载分发策略有两种：
RoundRobin：轮询模式，即轮询将请求转发到后端的各个pod上（默认模式）； SessionAffinity：基于客户端IP地址进行会话保持的模式，第一次客户端访问后端某个pod，之后的请求都转发到这个pod上。  三、服务发现 k8s服务发现方式 虽然Service解决了Pod的服务发现问题，但不提前知道Service的IP，怎么发现service服务呢？
k8s提供了两种方式进行服务发现：
● 环境变量： 当创建一个Pod的时候，kubelet会在该Pod中注入集群内所有Service的相关环境变量。需要注意的是，要想一个Pod中注入某个Service的环境变量，则必须Service要先比该Pod创建。这一点，几乎使得这种方式进行服务发现不可用。
例如： 一个ServiceName为redis-master的Service，对应的ClusterIP:Port为10.0.0.11:6379，则其在pod中对应的环境变量为： REDIS_MASTER_SERVICE_HOST=10.0.0.11 REDIS_MASTER_SERVICE_PORT=6379 REDIS_MASTER_PORT=tcp://10.0.0.11:6379 REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379 REDIS_MASTER_PORT_6379_TCP_PROTO=tcp REDIS_MASTER_PORT_6379_TCP_PORT=6379 REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11  ● DNS：可以通过cluster add-on的方式轻松的创建KubeDNS来对集群内的Service进行服务发现————这也是k8s官方强烈推荐的方式。为了让Pod中的容器可以使用kube-dns来解析域名，k8s会修改容器的/etc/resolv.conf配置。
k8s服务发现原理 ● endpoint
endpoint是k8s集群中的一个资源对象，存储在etcd中，用来记录一个service对应的所有pod的访问地址。
service配置selector，endpoint controller才会自动创建对应的endpoint对象；否则，不会生成endpoint对象.
例如，k8s集群中创建一个名为k8s-classic-1113-d3的service，就会生成一个同名的endpoint对象，如下图所示。其中ENDPOINTS就是service关联的pod的ip地址和端口。  ● endpoint controller
endpoint controller是k8s集群控制器的其中一个组件，其功能如下：
负责生成和维护所有endpoint对象的控制器 负责监听service和对应pod的变化 监听到service被删除，则删除和该service同名的endpoint对象 监听到新的service被创建，则根据新建service信息获取相关pod列表，然后创建对应endpoint对象 监听到service被更新，则根据更新后的service信息获取相关pod列表，然后更新对应endpoint对象 监听到pod事件，则更新对应的service的endpoint对象，将podIp记录到endpoint中  四、负载均衡 kube-proxy kube-proxy负责service的实现，即实现了k8s内部从pod到service和外部从node port到service的访问。</description>
    </item>
    
    <item>
      <title>Keycloak 配置SSL 实现HTTPS 访问</title>
      <link>https://yudar1024.github.io/blog/keycloak-ssl-configuration/</link>
      <pubDate>Sat, 21 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/keycloak-ssl-configuration/</guid>
      <description># 生成keystore.jks keytool -genkeypair -alias keycloak.me -keyalg RSA -keystore keycloak.jks -validity 10950   注意：CN必须是主机名，也可以是ip但是ip容易变，所以主机名或域名。后面访问的是时候通过在hosts 就文件映射域名到IP, 这里将CN 设置为keycloak.me, 注意alias参数不是设置CN的地方
 生成好keycloak.jks 之后 复制到 keycloak-7.0.0\standalone\configuration\ 文件夹内
修改standalone.xml 或 standalone-ha.xml 或 domain.xml 具体修哪个文件，看你的安装方式，此处使用standalone.xml
在 下添加
&amp;lt;security-realm name=&amp;quot;ApplicationRealm&amp;quot;&amp;gt; &amp;lt;server-identities&amp;gt; &amp;lt;ssl&amp;gt; &amp;lt;keystore path=&amp;quot;application.keystore&amp;quot; relative-to=&amp;quot;jboss.server.config.dir&amp;quot; keystore-password=&amp;quot;password&amp;quot; alias=&amp;quot;server&amp;quot; key-password=&amp;quot;password&amp;quot; generate-self-signed-certificate-host=&amp;quot;localhost&amp;quot; /&amp;gt; &amp;lt;/ssl&amp;gt; &amp;lt;/server-identities&amp;gt; &amp;lt;authentication&amp;gt; &amp;lt;local default-user=&amp;quot;$local&amp;quot; allowed-users=&amp;quot;*&amp;quot; skip-group-loading=&amp;quot;true&amp;quot; /&amp;gt; &amp;lt;properties path=&amp;quot;application-users.properties&amp;quot; relative-to=&amp;quot;jboss.server.config.dir&amp;quot; /&amp;gt; &amp;lt;/authentication&amp;gt; &amp;lt;authorization&amp;gt; &amp;lt;properties path=&amp;quot;application-roles.properties&amp;quot; relative-to=&amp;quot;jboss.server.config.dir&amp;quot; /&amp;gt; &amp;lt;/authorization&amp;gt; &amp;lt;/security-realm&amp;gt; &amp;lt;!-- 添加的部分 --&amp;gt; &amp;lt;security-realm name=&amp;quot;UndertowRealm&amp;quot;&amp;gt; &amp;lt;server-identities&amp;gt; &amp;lt;ssl&amp;gt; &amp;lt;keystore path=&amp;quot;keycloak.</description>
    </item>
    
    <item>
      <title>日常软件使用技巧</title>
      <link>https://yudar1024.github.io/blog/software-knowledge/</link>
      <pubDate>Tue, 17 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/software-knowledge/</guid>
      <description> 换行符问题(LF/CRLF) 将VScode， idea ， sublime text 设置为默认使用 linux 下的换行符
 vscode： 设置&amp;ndash;》用户设置&amp;ndash;》文本编辑器&amp;ndash;》文件&amp;ndash;》eol， 设置为\n 或者搜索 files:eol 进行设置 idea： File&amp;ndash;》Settings&amp;ndash;》Editor&amp;ndash;》Code Style&amp;ndash;》Line separator 设置为 Unix and OS X（\n） sublime text： Perference-&amp;gt;Setting-User 中加入配置 &amp;quot;default_line_ending&amp;quot;: &amp;quot;unix&amp;quot; 可选项为 system， windows， unix  </description>
    </item>
    
    <item>
      <title>使用 heketi 维护 glusterfs</title>
      <link>https://yudar1024.github.io/blog/using-heketi-to-maintain-glusterfs/</link>
      <pubDate>Wed, 11 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/using-heketi-to-maintain-glusterfs/</guid>
      <description>glusterfs 最日常的运维工作就是添加磁盘，添加节点。
添加磁盘  识别磁盘  linux 生产环境，不一定能够随时停机，此时就要求新添加的硬盘能够被热识别。如果是x86体系的物理硬件，必须断电停机，然后加硬盘。所以不存在问题，但是现在大多数环境是虚拟环境，所以添加硬盘非常方便。不需要断电，也就需要系统能够热识别。
此时linux服务器已经有sda 与sdb 两块磁盘，添加sdc磁盘（未识别，sdc是后面会识别出来的号）
# fdisk -l # 查看当前设备状态。此时看不到新添加的设备sdc # #进入/sys/class/scsi_host目录，在/sys/class/scsi_host下找到符合指向本机iscsi设备主机符号链接表 # ls -al /sys/class/scsi_host total 0 drwxr-xr-x. 2 root root 0 Nov 12 15:31 . drwxr-xr-x. 45 root root 0 Nov 12 15:31 .. lrwxrwxrwx. 1 root root 0 Nov 12 15:31 host0 -&amp;gt; ../../devices/pci0000:00/0000:00:07.1/host0/scsi_host/host0 lrwxrwxrwx. 1 root root 0 Nov 12 15:31 host1 -&amp;gt; ../../devices/pci0000:00/0000:00:07.1/host1/scsi_host/host1 lrwxrwxrwx. 1 root root 0 Nov 12 15:31 host2 -&amp;gt; .</description>
    </item>
    
    <item>
      <title>kubernetes 使用独立 glusterfs &#43; heketi集群</title>
      <link>https://yudar1024.github.io/blog/deploy-glusterfs-heketi-out-of-k8s/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/deploy-glusterfs-heketi-out-of-k8s/</guid>
      <description>主机列表
   Hostname 服务器IP 存储IP 用途 硬盘信息 容量G     master1 192.168.10.50 192.168.10.50 hekteti server /dev/sdb 300G   node1 192.168.10.51 192.168.10.51 glusterfs /dev/sdb 300G   node2 192.168.10.52 192.168.10.53 glusterfs /dev/sdb 300G   node3 192.168.10.53 192.168.10.53 glusterfs /dev/sdb 300G    所有主机的/etc/hosts均包含
192.168.10.50 master1 192.168.10.51 node1 192.168.10.52 node2 192.168.10.53 node3  配置所有服务器之间免密登录（可选）
在node1，node2，node3 安装 glusterfs-server glusterfs-fuse，并加载fuse 内核模块
#!/bin/bash yum install -y centos-release-gluster yum install -y glusterfs-server glusterfs-fuse systemctl start glusterd systemctl enable glusterd modprobe fuse echo &amp;quot;modprobe -- fuse&amp;quot; &amp;gt;&amp;gt; /etc/sysconfig/modules/glusterfs.</description>
    </item>
    
    <item>
      <title>kubernetes 集群中部署 glusterfs 与 heketi</title>
      <link>https://yudar1024.github.io/blog/deploy-glusterfs-hekti-in-k8s/</link>
      <pubDate>Sat, 07 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/deploy-glusterfs-hekti-in-k8s/</guid>
      <description>组件介绍 Heketi Heketi提供了一个RESTful管理界面，可以用来管理GlusterFS卷的生命周期。 通过Heketi，就可以像使用OpenStack Manila，Kubernetes和OpenShift一样申请可以动态配置GlusterFS卷。Heketi会动态在集群内选择bricks构建所需的volumes，这样以确保数据的副本会分散到集群不同的故障域内。同时Heketi还支持任意数量的ClusterFS集群，以保证接入的云服务器不局限于单个GlusterFS集群。
Gluster-Kubernetes Gluster-Kubernetes是一个可以将GluserFS和Hekiti轻松部署到Kubernetes集群的开源项目。另外也提供在Kubernetes中可以采用StorageClass来动态管理GlusterFS卷。
部署环境 服务器分配信息:
   Hostname 服务器IP 存储IP 硬盘信息 容量G     master1 192.168.10.51 192.168.10.51 /dev/sdb 300G   node1 192.168.10.52 192.168.10.52 /dev/sdb 300G   node2 192.168.10.53 192.168.10.53 /dev/sdb 300G    部署步骤 安装依赖组件 （master1 node1 node2）
yum install -y centos-release-gluster yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma glusterfs-geo-replication glusterfs-devel systemctl start glusterd systemctl enable glusterd  内核模块加载 （master1 node1 node2）</description>
    </item>
    
  </channel>
</rss>