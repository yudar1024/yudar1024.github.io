<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kubernetes on 陈sir 的流水帐</title>
    <link>https://yudar1024.github.io/categories/kubernetes/</link>
    <description>Recent content in kubernetes on 陈sir 的流水帐</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 11 Sep 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://yudar1024.github.io/categories/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>使用 heketi 维护 glusterfs</title>
      <link>https://yudar1024.github.io/blog/using-heketi-to-maintain-glusterfs/</link>
      <pubDate>Wed, 11 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/using-heketi-to-maintain-glusterfs/</guid>
      <description>glusterfs 最日常的运维工作就是添加磁盘，添加节点。
添加磁盘  识别磁盘  linux 生产环境，不一定能够随时停机，此时就要求新添加的硬盘能够被热识别。如果是x86体系的物理硬件，必须断电停机，然后加硬盘。所以不存在问题，但是现在大多数环境是虚拟环境，所以添加硬盘非常方便。不需要断电，也就需要系统能够热识别。
此时linux服务器已经有sda 与sdb 两块磁盘，添加sdc磁盘（未识别，sdc是后面会识别出来的号）
# fdisk -l # 查看当前设备状态。此时看不到新添加的设备sdc # #进入/sys/class/scsi_host目录，在/sys/class/scsi_host下找到符合指向本机iscsi设备主机符号链接表 # ls -al /sys/class/scsi_host total 0 drwxr-xr-x. 2 root root 0 Nov 12 15:31 . drwxr-xr-x. 45 root root 0 Nov 12 15:31 .. lrwxrwxrwx. 1 root root 0 Nov 12 15:31 host0 -&amp;gt; ../../devices/pci0000:00/0000:00:07.1/host0/scsi_host/host0 lrwxrwxrwx. 1 root root 0 Nov 12 15:31 host1 -&amp;gt; ../../devices/pci0000:00/0000:00:07.1/host1/scsi_host/host1 lrwxrwxrwx. 1 root root 0 Nov 12 15:31 host2 -&amp;gt; .</description>
    </item>
    
    <item>
      <title>kubernetes 使用独立 glusterfs &#43; heketi集群</title>
      <link>https://yudar1024.github.io/blog/deploy-glusterfs-heketi-out-of-k8s/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/deploy-glusterfs-heketi-out-of-k8s/</guid>
      <description>主机列表
   Hostname 服务器IP 存储IP 用途 硬盘信息 容量G     master1 192.168.10.50 192.168.10.50 hekteti server /dev/sdb 300G   node1 192.168.10.51 192.168.10.51 glusterfs /dev/sdb 300G   node2 192.168.10.52 192.168.10.53 glusterfs /dev/sdb 300G   node3 192.168.10.53 192.168.10.53 glusterfs /dev/sdb 300G    所有主机的/etc/hosts均包含
192.168.10.50 master1 192.168.10.51 node1 192.168.10.52 node2 192.168.10.53 node3  配置所有服务器之间免密登录（可选）
在node1，node2，node3 安装 glusterfs-server glusterfs-fuse，并加载fuse 内核模块
#!/bin/bash yum install -y centos-release-gluster yum install -y glusterfs-server glusterfs-fuse systemctl start glusterd systemctl enable glusterd modprobe fuse echo &amp;quot;modprobe -- fuse&amp;quot; &amp;gt;&amp;gt; /etc/sysconfig/modules/glusterfs.</description>
    </item>
    
    <item>
      <title>kubernetes 集群中部署 glusterfs 与 heketi</title>
      <link>https://yudar1024.github.io/blog/deploy-glusterfs-hekti-in-k8s/</link>
      <pubDate>Sat, 07 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/deploy-glusterfs-hekti-in-k8s/</guid>
      <description>组件介绍 Heketi Heketi提供了一个RESTful管理界面，可以用来管理GlusterFS卷的生命周期。 通过Heketi，就可以像使用OpenStack Manila，Kubernetes和OpenShift一样申请可以动态配置GlusterFS卷。Heketi会动态在集群内选择bricks构建所需的volumes，这样以确保数据的副本会分散到集群不同的故障域内。同时Heketi还支持任意数量的ClusterFS集群，以保证接入的云服务器不局限于单个GlusterFS集群。
Gluster-Kubernetes Gluster-Kubernetes是一个可以将GluserFS和Hekiti轻松部署到Kubernetes集群的开源项目。另外也提供在Kubernetes中可以采用StorageClass来动态管理GlusterFS卷。
部署环境 服务器分配信息:
   Hostname 服务器IP 存储IP 硬盘信息 容量G     master1 192.168.10.51 192.168.10.51 /dev/sdb 300G   node1 192.168.10.52 192.168.10.52 /dev/sdb 300G   node2 192.168.10.53 192.168.10.53 /dev/sdb 300G    部署步骤 安装依赖组件 （master1 node1 node2）
yum install -y centos-release-gluster yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma glusterfs-geo-replication glusterfs-devel systemctl start glusterd systemctl enable glusterd  内核模块加载 （master1 node1 node2）</description>
    </item>
    
  </channel>
</rss>