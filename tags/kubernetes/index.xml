<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kubernetes on 陈sir 的流水帐</title>
    <link>https://yudar1024.github.io/tags/kubernetes/</link>
    <description>Recent content in kubernetes on 陈sir 的流水帐</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 16 Oct 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://yudar1024.github.io/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>kubernetes 部署 rabbitmq 集群</title>
      <link>https://yudar1024.github.io/blog/deploy-rabbitmq-cluster/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/deploy-rabbitmq-cluster/</guid>
      <description>RABBITMQ 集群基础知识 基础概念  Producer: 消息发送者
 RabbitMQ
  :
 Vhost: 相当于分组，每个vhost下数据是隔离的 Exchange: 路由器，接收消息，本根据RoutingKey分发消息  headers：消息头类型 路由器，内部应用 direct：精准匹配类型 路由器 topic：主题匹配类型 路由器，支持正则 模糊匹配 fanout：广播类型 路由器，RoutingKey无效  RoutingKey: 路由规则 Queue: 队列，用于存储消息（消息的目的地）
 Consumer: 消息消费者
  持久化 RabbitMQ持久化分为Exchange、Queue、Message
 Exchange 和 Queue 持久化 指持久化Exchange、Queue 元数据，持久化的是自身，服务宕机，Exchange 和 Queue 自身就没有了 Message 持久化 顾名思义 把每一条消息体持久化，服务宕机，消息不丢失  镜像队列 rabbitmq的队列（queue）镜像，指master node 在接受到请求后，会同步到其他节点上，以此来保证高可用。在confirm模式下，具体过程如下
clientpublisher 发送消息&amp;ndash;&amp;gt; master node接到消息&amp;ndash;&amp;gt; master node 将消息持久化到磁盘 &amp;ndash;&amp;gt; 将消息异步发送给其他节点&amp;ndash;&amp;gt;master 将ack 返回给client publisher。</description>
    </item>
    
    <item>
      <title>kubernetes service 介绍</title>
      <link>https://yudar1024.github.io/blog/service-introduction/</link>
      <pubDate>Mon, 23 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/service-introduction/</guid>
      <description>什么是service 是发现后端pod服务；
是为一组具有相同功能的容器应用提供一个统一的入口地址；
是将请求进行负载分发到后端的各个容器应用上的控制器。
对service的访问来源 访问service的请求来源有两种：k8s集群内部的程序（Pod）和 k8s集群外部的程序。
service类型 采用微服务架构时，作为服务所有者，除了实现业务逻辑以外，还需要考虑如何把服务发布到k8s集群或者集群外部，使这些服务能够被k8s集群内的应用、其他k8s集群的应用以及外部应用使用。因此k8s提供了灵活的服务发布方式，用户可以通过ServiceType来指定如何来发布服务，类型有以下几种：
● ClusterIP：提供一个集群内部的虚拟IP以供Pod访问（service默认类型)。
service 结构如下：  ● NodePort:在每个Node上打开一个端口以供外部访问
Kubernetes将会在每个Node上打开一个端口并且每个Node的端口都是一样的，通过:NodePort的方式Kubernetes集群外部的程序可以访问Service。
service 定义如下：  ● LoadBalancer：通过外部的负载均衡器来访问
service selector service通过selector和pod建立关联。
k8s会根据service关联到pod的podIP信息组合成一个endpoint。
若service定义中没有selector字段，service被创建时，endpoint controller不会自动创建endpoint。
service负载分发策略 service 负载分发策略有两种：
RoundRobin：轮询模式，即轮询将请求转发到后端的各个pod上（默认模式）； SessionAffinity：基于客户端IP地址进行会话保持的模式，第一次客户端访问后端某个pod，之后的请求都转发到这个pod上。  三、服务发现 k8s服务发现方式 虽然Service解决了Pod的服务发现问题，但不提前知道Service的IP，怎么发现service服务呢？
k8s提供了两种方式进行服务发现：
● 环境变量： 当创建一个Pod的时候，kubelet会在该Pod中注入集群内所有Service的相关环境变量。需要注意的是，要想一个Pod中注入某个Service的环境变量，则必须Service要先比该Pod创建。这一点，几乎使得这种方式进行服务发现不可用。
例如： 一个ServiceName为redis-master的Service，对应的ClusterIP:Port为10.0.0.11:6379，则其在pod中对应的环境变量为： REDIS_MASTER_SERVICE_HOST=10.0.0.11 REDIS_MASTER_SERVICE_PORT=6379 REDIS_MASTER_PORT=tcp://10.0.0.11:6379 REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379 REDIS_MASTER_PORT_6379_TCP_PROTO=tcp REDIS_MASTER_PORT_6379_TCP_PORT=6379 REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11  ● DNS：可以通过cluster add-on的方式轻松的创建KubeDNS来对集群内的Service进行服务发现————这也是k8s官方强烈推荐的方式。为了让Pod中的容器可以使用kube-dns来解析域名，k8s会修改容器的/etc/resolv.conf配置。
k8s服务发现原理 ● endpoint
endpoint是k8s集群中的一个资源对象，存储在etcd中，用来记录一个service对应的所有pod的访问地址。
service配置selector，endpoint controller才会自动创建对应的endpoint对象；否则，不会生成endpoint对象.
例如，k8s集群中创建一个名为k8s-classic-1113-d3的service，就会生成一个同名的endpoint对象，如下图所示。其中ENDPOINTS就是service关联的pod的ip地址和端口。  ● endpoint controller
endpoint controller是k8s集群控制器的其中一个组件，其功能如下：
负责生成和维护所有endpoint对象的控制器 负责监听service和对应pod的变化 监听到service被删除，则删除和该service同名的endpoint对象 监听到新的service被创建，则根据新建service信息获取相关pod列表，然后创建对应endpoint对象 监听到service被更新，则根据更新后的service信息获取相关pod列表，然后更新对应endpoint对象 监听到pod事件，则更新对应的service的endpoint对象，将podIp记录到endpoint中  四、负载均衡 kube-proxy kube-proxy负责service的实现，即实现了k8s内部从pod到service和外部从node port到service的访问。</description>
    </item>
    
    <item>
      <title>使用 heketi 维护 glusterfs</title>
      <link>https://yudar1024.github.io/blog/using-heketi-to-maintain-glusterfs/</link>
      <pubDate>Wed, 11 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/using-heketi-to-maintain-glusterfs/</guid>
      <description>glusterfs 最日常的运维工作就是添加磁盘，添加节点。
添加磁盘  识别磁盘  linux 生产环境，不一定能够随时停机，此时就要求新添加的硬盘能够被热识别。如果是x86体系的物理硬件，必须断电停机，然后加硬盘。所以不存在问题，但是现在大多数环境是虚拟环境，所以添加硬盘非常方便。不需要断电，也就需要系统能够热识别。
此时linux服务器已经有sda 与sdb 两块磁盘，添加sdc磁盘（未识别，sdc是后面会识别出来的号）
# fdisk -l # 查看当前设备状态。此时看不到新添加的设备sdc # #进入/sys/class/scsi_host目录，在/sys/class/scsi_host下找到符合指向本机iscsi设备主机符号链接表 # ls -al /sys/class/scsi_host total 0 drwxr-xr-x. 2 root root 0 Nov 12 15:31 . drwxr-xr-x. 45 root root 0 Nov 12 15:31 .. lrwxrwxrwx. 1 root root 0 Nov 12 15:31 host0 -&amp;gt; ../../devices/pci0000:00/0000:00:07.1/host0/scsi_host/host0 lrwxrwxrwx. 1 root root 0 Nov 12 15:31 host1 -&amp;gt; ../../devices/pci0000:00/0000:00:07.1/host1/scsi_host/host1 lrwxrwxrwx. 1 root root 0 Nov 12 15:31 host2 -&amp;gt; .</description>
    </item>
    
    <item>
      <title>kubernetes 使用独立 glusterfs &#43; heketi集群</title>
      <link>https://yudar1024.github.io/blog/deploy-glusterfs-heketi-out-of-k8s/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/deploy-glusterfs-heketi-out-of-k8s/</guid>
      <description>主机列表
   Hostname 服务器IP 存储IP 用途 硬盘信息 容量G     master1 192.168.10.50 192.168.10.50 hekteti server /dev/sdb 300G   node1 192.168.10.51 192.168.10.51 glusterfs /dev/sdb 300G   node2 192.168.10.52 192.168.10.53 glusterfs /dev/sdb 300G   node3 192.168.10.53 192.168.10.53 glusterfs /dev/sdb 300G    所有主机的/etc/hosts均包含
192.168.10.50 master1 192.168.10.51 node1 192.168.10.52 node2 192.168.10.53 node3  配置所有服务器之间免密登录（可选）
在node1，node2，node3 安装 glusterfs-server glusterfs-fuse，并加载fuse 内核模块
#!/bin/bash yum install -y centos-release-gluster yum install -y glusterfs-server glusterfs-fuse systemctl start glusterd systemctl enable glusterd modprobe fuse echo &amp;quot;modprobe -- fuse&amp;quot; &amp;gt;&amp;gt; /etc/sysconfig/modules/glusterfs.</description>
    </item>
    
    <item>
      <title>kubernetes 集群中部署 glusterfs 与 heketi</title>
      <link>https://yudar1024.github.io/blog/deploy-glusterfs-hekti-in-k8s/</link>
      <pubDate>Sat, 07 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yudar1024.github.io/blog/deploy-glusterfs-hekti-in-k8s/</guid>
      <description>组件介绍 Heketi Heketi提供了一个RESTful管理界面，可以用来管理GlusterFS卷的生命周期。 通过Heketi，就可以像使用OpenStack Manila，Kubernetes和OpenShift一样申请可以动态配置GlusterFS卷。Heketi会动态在集群内选择bricks构建所需的volumes，这样以确保数据的副本会分散到集群不同的故障域内。同时Heketi还支持任意数量的ClusterFS集群，以保证接入的云服务器不局限于单个GlusterFS集群。
Gluster-Kubernetes Gluster-Kubernetes是一个可以将GluserFS和Hekiti轻松部署到Kubernetes集群的开源项目。另外也提供在Kubernetes中可以采用StorageClass来动态管理GlusterFS卷。
部署环境 服务器分配信息:
   Hostname 服务器IP 存储IP 硬盘信息 容量G     master1 192.168.10.51 192.168.10.51 /dev/sdb 300G   node1 192.168.10.52 192.168.10.52 /dev/sdb 300G   node2 192.168.10.53 192.168.10.53 /dev/sdb 300G    部署步骤 安装依赖组件 （master1 node1 node2）
yum install -y centos-release-gluster yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma glusterfs-geo-replication glusterfs-devel systemctl start glusterd systemctl enable glusterd  内核模块加载 （master1 node1 node2）</description>
    </item>
    
  </channel>
</rss>